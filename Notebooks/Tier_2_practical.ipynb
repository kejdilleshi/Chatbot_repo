{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925cdbd2",
   "metadata": {},
   "source": [
    "# Practical Session: From Pre-trained transformers to Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06846d44",
   "metadata": {},
   "source": [
    "![Chatbot Illustration !](Images/chatbot.jpg \"Designed by www.freepik.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1da823",
   "metadata": {},
   "source": [
    "\n",
    "### Objective\n",
    "\n",
    "Welcome to our hands-on session, **\"From Pre-trained to Chatbot\"**! In this practical session, we'll transform a pre-trained GPT-2 model into a functional chatbot using the `multi_woz_v22` dataset. By the end of this workshop, you will have gained hands-on experience in:\n",
    "\n",
    "- Understanding the mechanics behind a transformer-based chatbot.\n",
    "- Preprocessing and formatting dialogue data for training.\n",
    "- Fine-tuning GPT-2 to generate meaningful, task-oriented dialogue.\n",
    "\n",
    "Ready? Let's dive in!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023c7f1",
   "metadata": {},
   "source": [
    "#### Necessary steps, run the hidden cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbdbbf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "from transformers import Trainer, TrainingArguments, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import logging as transformers_logging\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from torchinfo import summary\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import numpy\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "from packaging import version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e082ac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Check:\n",
      "python: 3.10.15 (âœ”)\n",
      "torch: 2.3.0+cu121 (âœ”)\n",
      "transformers: 4.41.2 (âœ”)\n",
      "datasets: 2.19.2 (âœ”)\n",
      "numpy: 1.26.4 (âœ”)\n"
     ]
    }
   ],
   "source": [
    "# Confirm you have the right versions \n",
    "# Minimum required versions\n",
    "required_versions = {\n",
    "    \"python\": \"3.10.15\",\n",
    "    \"torch\": \"2.3.0\",\n",
    "    \"transformers\": \"4.41.2\",\n",
    "    \"datasets\": \"2.19.2\",\n",
    "    \"numpy\": \"1.26.4\"\n",
    "}\n",
    "\n",
    "# Fetch current versions\n",
    "current_versions = {\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"torch\": torch.__version__,\n",
    "    \"transformers\": transformers.__version__,\n",
    "    \"datasets\": datasets.__version__,\n",
    "    \"numpy\": np.__version__\n",
    "}\n",
    "\n",
    "print(\"Version Check:\")\n",
    "for package, required_version in required_versions.items():\n",
    "    current_version = current_versions[package]\n",
    "    \n",
    "    # Compare versions\n",
    "    if version.parse(current_version) >= version.parse(required_version):\n",
    "        print(f\"{package}: {current_version} (âœ”)\")\n",
    "    else:\n",
    "        print(f\"{package}: {current_version} (âœ˜) - Requires: {required_version} or later\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd086708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paths to the directories to load and save datasets and models:\n",
    "\n",
    "platform = \"laptop\" # possible choices are \"jupyter\", \"laptop\" and \"curnagl\"\n",
    "\n",
    "if platform == \"jupyter\":\n",
    "    PATH_DATASETS = \"./Datasets/\"\n",
    "    PATH_MODELS = \"./Models/\"\n",
    "    PATH_RESULTS = \"./Results/\"\n",
    "    \n",
    "elif platform == \"laptop\":\n",
    "    PATH_DATASETS = \"./Datasets/\"\n",
    "    PATH_MODELS = \"./Models/\"\n",
    "    PATH_RESULTS = \"./Results/\"\n",
    "    \n",
    "elif platform == \"curnagl\":\n",
    "    PATH_DATASETS = \"/users/klleshi/LLMProject/chatbot\"\n",
    "    PATH_MODELS = \"/users/klleshi/LLMProject/chatbot\"\n",
    "    PATH_RESULTS = \"/users/klleshi/LLMProject/chatbot\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb15f742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Try to use GPUs if available:\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Suppress some specific logging and warnings from transformers and Hugging Face Hub modules:\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.file_download\")\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35fdd489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed, to make the whole pipeline reproducible:\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "transformers.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad66cb3d",
   "metadata": {},
   "source": [
    "## 1. Our Dialogue Dataset: `multi_woz_v22`\n",
    "\n",
    "The `multi_woz_v22` dataset is a comprehensive, multi-domain dialogue dataset, perfect for training a task-oriented chatbot. It contains over 10,000 dialogues spanning several domains, such as **restaurant booking, hotel reservation, taxi services, and more**.\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "Each dialogue in [`multi_woz_v22`](https://huggingface.co/datasets/pfb30/multi_woz_v22)  contains:\n",
    "- **User Utterances**: What the user says, e.g., \"Can you book a hotel for me?\"\n",
    "- **System Responses**: What the system replies, e.g., \"Sure, which city are you looking for?\"\n",
    "- **Metadata**: Information about the domain, dialogue state, and more.\n",
    "\n",
    "Let's visualize a sample dialogue from the dataset to better understand it:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the multi_woz_v22 dataset\n",
    "dataset = load_dataset(\"multi_woz_v22\")\n",
    "print(dataset['train'][0])  # Display the first example in the training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb21349-0166-4bc2-a178-574ecc091b95",
   "metadata": {},
   "source": [
    "We are going to use the Multi-Domain Wizard-of-Oz dataset (MultiWOZ). This is a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. See https://huggingface.co/datasets/pfb30/multi_woz_v22 and https://arxiv.org/abs/1810.00278.\n",
    "\n",
    "Load the \"MultiWOZ\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ee88a9-39ee-43b9-8123-cc82fbc15cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('multi_woz_v22', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dff9df8b-f7f2-4920-afc9-dd511d35bbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dialogue_id', 'services', 'turns'],\n",
       "        num_rows: 8437\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['dialogue_id', 'services', 'turns'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['dialogue_id', 'services', 'turns'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84cfc5d6-1cfa-4a31-9014-20626769b6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dialogue_id': 'PMUL4398.json', 'services': ['restaurant', 'hotel'], 'turns': {'turn_id': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11'], 'speaker': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], 'utterance': ['i need a place to dine in the center thats expensive', 'I have several options for you; do you prefer African, Asian, or British food?', 'Any sort of food would be fine, as long as it is a bit expensive. Could I get the phone number for your recommendation?', 'There is an Afrian place named Bedouin in the centre. How does that sound?', 'Sounds good, could I get that phone number? Also, could you recommend me an expensive hotel?', \"Bedouin's phone is 01223367660. As far as hotels go, I recommend the University Arms Hotel in the center of town.\", 'Yes. Can you book it for me?', 'Sure, when would you like that reservation?', 'i want to book it for 2 people and 2 nights starting from saturday.', 'Your booking was successful. Your reference number is FRGZWQL2 . May I help you further?', 'That is all I need to know. Thanks, good bye.', 'Thank you so much for Cambridge TownInfo centre. Have a great day!'], 'frames': [{'service': ['restaurant', 'hotel'], 'state': [{'active_intent': 'find_restaurant', 'requested_slots': [], 'slots_values': {'slots_values_name': ['restaurant-area', 'restaurant-pricerange'], 'slots_values_list': [['centre'], ['expensive']]}}, {'active_intent': 'find_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': [], 'slots_values_list': []}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}, {'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': ['restaurant', 'hotel'], 'state': [{'active_intent': 'find_restaurant', 'requested_slots': ['restaurant-food'], 'slots_values': {'slots_values_name': ['restaurant-area', 'restaurant-pricerange'], 'slots_values_list': [['centre'], ['expensive']]}}, {'active_intent': 'find_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': [], 'slots_values_list': []}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}, {'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': ['restaurant', 'hotel'], 'state': [{'active_intent': 'find_restaurant', 'requested_slots': ['restaurant-phone'], 'slots_values': {'slots_values_name': ['restaurant-area', 'restaurant-name', 'restaurant-pricerange'], 'slots_values_list': [['centre'], ['bedouin'], ['expensive']]}}, {'active_intent': 'find_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': ['hotel-pricerange', 'hotel-type'], 'slots_values_list': [['expensive'], ['hotel']]}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}, {'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': ['hotel'], 'state': [{'active_intent': 'find_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': ['hotel-name', 'hotel-pricerange', 'hotel-type'], 'slots_values_list': [['university arms hotel'], ['expensive'], ['hotel']]}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': ['hotel'], 'state': [{'active_intent': 'book_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': ['hotel-bookday', 'hotel-bookpeople', 'hotel-bookstay', 'hotel-name', 'hotel-pricerange', 'hotel-type'], 'slots_values_list': [['saturday'], ['2'], ['2'], ['university arms hotel'], ['expensive'], ['hotel']]}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': [], 'state': [], 'slots': []}, {'service': [], 'state': [], 'slots': []}], 'dialogue_acts': [{'dialog_act': {'act_type': ['Restaurant-Inform'], 'act_slots': [{'slot_name': ['area', 'pricerange'], 'slot_value': ['centre', 'expensive']}]}, 'span_info': {'act_type': ['Restaurant-Inform', 'Restaurant-Inform'], 'act_slot_name': ['area', 'pricerange'], 'act_slot_value': ['centre', 'expensive'], 'span_start': [30, 43], 'span_end': [36, 52]}}, {'dialog_act': {'act_type': ['Restaurant-Inform', 'Restaurant-Select'], 'act_slots': [{'slot_name': ['choice'], 'slot_value': ['several']}, {'slot_name': ['food', 'food', 'food'], 'slot_value': ['African', 'Asian', 'British']}]}, 'span_info': {'act_type': ['Restaurant-Inform', 'Restaurant-Select', 'Restaurant-Select', 'Restaurant-Select'], 'act_slot_name': ['choice', 'food', 'food', 'food'], 'act_slot_value': ['several', 'African', 'Asian', 'British'], 'span_start': [7, 46, 55, 65], 'span_end': [14, 53, 60, 72]}}, {'dialog_act': {'act_type': ['Restaurant-Request'], 'act_slots': [{'slot_name': ['food'], 'slot_value': ['?']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}, {'dialog_act': {'act_type': ['Restaurant-Inform'], 'act_slots': [{'slot_name': ['area', 'food', 'name'], 'slot_value': ['centre', 'Afrian', 'Bedouin']}]}, 'span_info': {'act_type': ['Restaurant-Inform', 'Restaurant-Inform', 'Restaurant-Inform'], 'act_slot_name': ['food', 'name', 'area'], 'act_slot_value': ['Afrian', 'Bedouin', 'centre'], 'span_start': [12, 31, 46], 'span_end': [18, 38, 52]}}, {'dialog_act': {'act_type': ['Hotel-Inform', 'Restaurant-Request'], 'act_slots': [{'slot_name': ['pricerange', 'type'], 'slot_value': ['expensive', 'hotel']}, {'slot_name': ['phone'], 'slot_value': ['?']}]}, 'span_info': {'act_type': ['Hotel-Inform', 'Hotel-Inform'], 'act_slot_name': ['pricerange', 'type'], 'act_slot_value': ['expensive', 'hotel'], 'span_start': [76, 86], 'span_end': [85, 91]}}, {'dialog_act': {'act_type': ['Hotel-Recommend', 'Restaurant-Inform'], 'act_slots': [{'slot_name': ['area', 'name'], 'slot_value': ['center of town', 'the University Arms Hotel']}, {'slot_name': ['name', 'phone'], 'slot_value': ['Bedouin', '01223367660']}]}, 'span_info': {'act_type': ['Restaurant-Inform', 'Restaurant-Inform', 'Hotel-Recommend', 'Hotel-Recommend'], 'act_slot_name': ['name', 'phone', 'name', 'area'], 'act_slot_value': ['Bedouin', '01223367660', 'the University Arms Hotel', 'center of town'], 'span_start': [0, 19, 65, 98], 'span_end': [7, 30, 90, 112]}}, {'dialog_act': {'act_type': ['Hotel-Inform'], 'act_slots': [{'slot_name': ['none'], 'slot_value': ['none']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}, {'dialog_act': {'act_type': ['Booking-Request'], 'act_slots': [{'slot_name': ['bookday'], 'slot_value': ['?']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}, {'dialog_act': {'act_type': ['Hotel-Inform'], 'act_slots': [{'slot_name': ['bookday', 'bookpeople', 'bookstay'], 'slot_value': ['saturday', '2', '2']}]}, 'span_info': {'act_type': ['Hotel-Inform', 'Hotel-Inform', 'Hotel-Inform'], 'act_slot_name': ['bookstay', 'bookpeople', 'bookday'], 'act_slot_value': ['2', '2', 'saturday'], 'span_start': [22, 35, 58], 'span_end': [23, 36, 66]}}, {'dialog_act': {'act_type': ['Booking-Book', 'general-reqmore'], 'act_slots': [{'slot_name': ['ref'], 'slot_value': ['FRGZWQL2']}, {'slot_name': ['none'], 'slot_value': ['none']}]}, 'span_info': {'act_type': ['Booking-Book'], 'act_slot_name': ['ref'], 'act_slot_value': ['FRGZWQL2'], 'span_start': [54], 'span_end': [62]}}, {'dialog_act': {'act_type': ['general-bye'], 'act_slots': [{'slot_name': ['none'], 'slot_value': ['none']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}, {'dialog_act': {'act_type': ['general-bye', 'general-welcome'], 'act_slots': [{'slot_name': ['none'], 'slot_value': ['none']}, {'slot_name': ['none'], 'slot_value': ['none']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}]}}\n"
     ]
    }
   ],
   "source": [
    "# Look at an example:\n",
    "example = dataset['train'][0]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489c09e-2993-4853-aa18-e493af208a60",
   "metadata": {},
   "source": [
    "### 1.2 Special Tokens and Why They Matter\n",
    "\n",
    "When working with conversational data, itâ€™s crucial to use special tokens to help the model understand the structure of the dialogue. These tokens act as **guidelines** for the model to differentiate between user and system turns and know when the conversation ends.\n",
    "\n",
    "#### Our Special Tokens\n",
    "\n",
    "We'll be using the following special tokens:\n",
    "\n",
    "- **`<|user|>`**: Indicates the beginning of a user utterance.\n",
    "- **`<|system|>`**: Indicates the beginning of a system (chatbot) response.\n",
    "- **`<|endofturn|>`**: Marks the end of each dialogue turn.\n",
    "- **`<|pad|>`**: Used to pad sequences to the same length for batch processing.\n",
    "\n",
    "#### Why Are Special Tokens Important?\n",
    "\n",
    "1. **Clarifying Speaker Roles**: In a conversation, distinguishing who is speaking (user or system) is essential for the model to learn appropriate response patterns.\n",
    "2. **Defining Turn Boundaries**: By marking the end of a dialogue turn, we ensure the model can process and structure conversations more effectively.\n",
    "3. **Efficient Padding**: The `<|pad|>` token ensures that input sequences are uniformly sized, making batch processing efficient and reducing computational load.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077d1b60-2427-45b5-87c5-fba8c73d23aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [USER] i need a place to dine in the center thats expensive \n",
      " [BOT] I have several options for you; do you prefer African, Asian, or British food?\n",
      " [USER] Any sort of food would be fine, as long as it is a bit expensive. Could I get the phone number for your recommendation? \n",
      " [BOT] There is an Afrian place named Bedouin in the centre. How does that sound?\n",
      " [USER] Sounds good, could I get that phone number? Also, could you recommend me an expensive hotel? \n",
      " [BOT] Bedouin's phone is 01223367660. As far as hotels go, I recommend the University Arms Hotel in the center of town.\n",
      " [USER] Yes. Can you book it for me? \n",
      " [BOT] Sure, when would you like that reservation?\n",
      " [USER] i want to book it for 2 people and 2 nights starting from saturday. \n",
      " [BOT] Your booking was successful. Your reference number is FRGZWQL2 . May I help you further?\n",
      " [USER] That is all I need to know. Thanks, good bye. \n",
      " [BOT] Thank you so much for Cambridge TownInfo centre. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "# Visualize a conversation in the dataset.\n",
    "\n",
    "dialogue = example[\"turns\"]\n",
    "\n",
    "for turn_id, speaker, utterance in zip(dialogue[\"turn_id\"], dialogue[\"speaker\"], dialogue[\"utterance\"]):\n",
    "    if speaker == 0:  # User input\n",
    "        print(f\" [USER] {utterance.strip()} \")\n",
    "    elif speaker == 1:  # System response\n",
    "        print(f\" [BOT] {utterance.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb6f356-45a6-49d8-8c80-9af26deb2e67",
   "metadata": {},
   "source": [
    "Add the special token \"<|endoftext|>\" at the end of each diologue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7282b38d-e4f5-45b6-bc48-de9126a6c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[USER] i need a place to dine in the center thats expensive [BOT] I have several options for you; do you prefer African, Asian, or British food? [USER] Any sort of food would be fine, as long as it is a bit expensive. Could I get the phone number for your recommendation? [BOT] There is an Afrian place named Bedouin in the centre. How does that sound? [USER] Sounds good, could I get that phone number? Also, could you recommend me an expensive hotel? [BOT] Bedouin's phone is 01223367660. As far as hotels go, I recommend the University Arms Hotel in the center of town. [USER] Yes. Can you book it for me? [BOT] Sure, when would you like that reservation? [USER] i want to book it for 2 people and 2 nights starting from saturday. [BOT] Your booking was successful. Your reference number is FRGZWQL2 . May I help you further? [USER] That is all I need to know. Thanks, good bye. [BOT] Thank you so much for Cambridge TownInfo centre. Have a great day! <|endoftext|>\", \"[USER] Guten Tag, I am staying overnight in Cambridge and need a place to sleep. I need free parking and internet. [BOT] I have 4 different options for you. I have two cheaper guesthouses and two expensive hotels. Do you have a preference? [USER] No, but I'd really like to be on the south end of the city. Do any of those fit the bill? [BOT] Sure. Does price matter? We can narrow it down and find exactly what you need. [USER] No I don't care about the price. Which one do you recommend? [BOT] I would recommend aylesbray lodge guest house. Would you like me to book that for you? [USER] Yes, book it for 4 people and 4 nights starting from tuesday. [BOT] The booking was unsuccessful. would you like another day or a shorter stay? [USER] How about for 3 nights? [BOT] Booked! Reference number is: 84ESP6F5 [USER] Great. I am all set then. Have a nice day. Bye. [BOT] Have a nice stay. Bye. <|endoftext|>\"]\n"
     ]
    }
   ],
   "source": [
    "examples = dataset['train'].select(range(2))\n",
    "\n",
    "dialogues = []\n",
    "for dialogue in examples['turns']:\n",
    "    dialogue_text = \"\"\n",
    "    for turn_id, speaker, utterance in zip(dialogue[\"turn_id\"], dialogue[\"speaker\"], dialogue[\"utterance\"]):\n",
    "        if speaker == 0:  # User input\n",
    "            dialogue_text += f\"[USER] {utterance.strip()} \"\n",
    "        elif speaker == 1:  # System response\n",
    "            dialogue_text += f\"[BOT] {utterance.strip()} \"\n",
    "    dialogues.append(dialogue_text + \"<|endoftext|>\")\n",
    "\n",
    "print(dialogues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5382ffd4-09bb-48ec-8779-e4b0ab67d14f",
   "metadata": {},
   "source": [
    "Load the pre-trained GPT-2 tokenizer and add special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25b632d8-92ca-4383-a4e2-35f97a9216d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "special_tokens_dict = {'additional_special_tokens': ['[USER]', '[BOT]'], 'pad_token': '[PAD]'}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1bcc3-48b1-42f3-8a11-ee242b088479",
   "metadata": {},
   "source": [
    "#### Example Explanation\n",
    "\n",
    "Here the special tokens `'<|user|>'` and `'<|system|>'` help GPT-2 to differentiate between different speakers in a conversation, facilitating the modelâ€™s ability to learn how a dialogue typically unfolds. By labeling input text with `'<|user|>'` for the humanâ€™s part of the conversation and `'<|system|>'` for the chatbotâ€™s response, you give the model a clear format. This will help it understand conversational roles and can also improve the training of dialogue models.\n",
    "\n",
    "GPT-2 will learn the conversational flow, that is the back-and-forth nature of conversation, and be able to generate coherent dialogue after training. In summary, GPT-2 will generate next words in such a way to mimic a conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8f4df-1b21-45af-bb8a-77c5d438cdf7",
   "metadata": {},
   "source": [
    "Analyse the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ae31ed3-79ad-4141-91f0-219e79bf850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = dataset['train']\n",
    "\n",
    "dialogues = []\n",
    "for dialogue in examples['turns']:\n",
    "    dialogue_text = \"\"\n",
    "    for turn_id, speaker, utterance in zip(dialogue[\"turn_id\"], dialogue[\"speaker\"], dialogue[\"utterance\"]):\n",
    "        if speaker == 0:  # User input\n",
    "            dialogue_text += f\"[USER] {utterance.strip()} \"\n",
    "        elif speaker == 1:  # System response\n",
    "            dialogue_text += f\"[BOT] {utterance.strip()} \"\n",
    "    dialogues.append(dialogue_text + \"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16c78d47-523c-4c08-afe9-c93538568461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of dialogues in the dataset:  8437\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of dialogues in the dataset: \", len(dialogues))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e37095-2c1c-4613-be88-598d1b1e0724",
   "metadata": {},
   "source": [
    "Plot a histogram of the number of tokens in each dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a5c5fd-5cf1-4e36-9983-b066f1e449ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dialogues' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Show the plot\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 39\u001b[0m plot_token_histogram(\u001b[43mdialogues\u001b[49m, tokenizer, num_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dialogues' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_token_histogram(dialogues, tokenizer, num_bins=10):\n",
    "\n",
    "    # Tokenizing the dialogues and counting tokens in each dialogue\n",
    "    num_tokens_per_dialogue = [len(tokenizer.tokenize(dialogue)) for dialogue in dialogues]\n",
    "\n",
    "    # Create histogram bins\n",
    "    token_min = min(num_tokens_per_dialogue)\n",
    "    token_max = max(num_tokens_per_dialogue)\n",
    "    bins = np.linspace(token_min, token_max, num_bins + 1)  # Create `num_bins` equally spaced bins\n",
    "\n",
    "    # Count occurrences of each token count\n",
    "    token_counts, _ = np.histogram(num_tokens_per_dialogue, bins=bins)\n",
    "\n",
    "    # Count occurrences of [USER] in each dialogue\n",
    "    num_user_tokens_per_dialogue = [dialogue.count('[USER]') for dialogue in dialogues]\n",
    "    user_token_counts = Counter(num_user_tokens_per_dialogue)\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot the histogram of token counts using bins\n",
    "    axs[0].bar(bins[:-1], token_counts, width=np.diff(bins), align='edge')  # Plot with bins\n",
    "    axs[0].set_xlabel('Number of Tokens')\n",
    "    axs[0].set_ylabel('Number of Dialogues')\n",
    "    axs[0].set_title('Histogram of Number of Tokens per Dialogue')\n",
    "\n",
    "    # Plot the histogram of [USER] token counts\n",
    "    axs[1].bar(user_token_counts.keys(), user_token_counts.values())\n",
    "    axs[1].set_xlabel('Number of [USER] Tokens')\n",
    "    axs[1].set_ylabel('Number of Dialogues')\n",
    "    axs[1].set_title('Histogram of Number of [USER] Tokens per Dialogue')\n",
    "\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_token_histogram(dialogues, tokenizer, num_bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb482dd1-d6a9-470e-992f-985d3fd43fff",
   "metadata": {},
   "source": [
    "Prepare the data in a dialogue format with maximum 512 tokens per conversation (this number may be increased to allow longer conversations) and maximum 12 user-bot exchanges per conversation. \n",
    "\n",
    "Terminology:\n",
    "- 1 turn = 1 message (either from the user or the bot)\n",
    "- 1 user-bot exchange = 2-turn conversation (1 message from the user followed by 1 message from the bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66301873-1915-4bb6-99bd-48f0d627c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_balanced(examples, max_context_exchanges=12):\n",
    "    \"\"\"\n",
    "    This function processes dialogues and generates a balanced mix of single-turn and multi-turn conversations.\n",
    "    Handles up to 12 user-bot exchanges (24 turns) in conversations within 512 tokens, ensuring efficient use of context.\n",
    "\n",
    "    Args:\n",
    "    - examples: The dataset examples containing dialogues.\n",
    "    - max_context_exchanges: Maximum number of user-bot exchanges to include in the sliding window (12 user-bot exchanges).\n",
    "\n",
    "    Returns:\n",
    "    - tokenized_inputs: Tokenized inputs for training the model.\n",
    "    \"\"\"\n",
    "    dialogues = []\n",
    "\n",
    "    for dialogue in examples['turns']:\n",
    "        num_turns = len(dialogue[\"utterance\"])\n",
    "\n",
    "        # Step 1: Single-turn dialogue (focus on concise bot responses, 1 user-bot exchange = 2 turns)\n",
    "        for i in range(0, num_turns - 1, 2):  # Each user-bot exchange is 2 turns\n",
    "            if dialogue[\"speaker\"][i] == 0 and dialogue[\"speaker\"][i + 1] == 1:\n",
    "                user_utterance = dialogue[\"utterance\"][i].strip()\n",
    "                bot_response = dialogue[\"utterance\"][i + 1].strip()\n",
    "                single_turn = f\"[USER] {user_utterance} [BOT] {bot_response} <|endoftext|>\"\n",
    "                dialogues.append(single_turn)\n",
    "\n",
    "        # Step 2: Multi-turn conversations (sliding window approach with only 1 user-bot exchange overlap)\n",
    "        # Select windows starting with at least 2 user-bot exchanges (4 turns) up to max_context_exchanges\n",
    "        for num_exchanges in range(2, min(max_context_exchanges + 1, num_turns // 2 + 1)):  # Sliding window in terms of user-bot exchanges\n",
    "            for start_turn_index in range(0, num_turns - (num_exchanges * 2) + 1, 2 * (num_exchanges - 1)):  # Ensure valid window sizes\n",
    "                dialogue_text = \"\"\n",
    "                for exchange_index in range(num_exchanges):  # Each iteration captures 1 user-bot exchange (2 turns)\n",
    "                    user_utterance = dialogue[\"utterance\"][start_turn_index + 2 * exchange_index].strip()\n",
    "                    bot_response = dialogue[\"utterance\"][start_turn_index + 2 * exchange_index + 1].strip()\n",
    "                    dialogue_text += f\"[USER] {user_utterance} [BOT] {bot_response} \"\n",
    "                dialogues.append(dialogue_text + \"<|endoftext|>\")\n",
    "\n",
    "    # Tokenize the combined dialogue list\n",
    "    tokenized_inputs = tokenizer(dialogues, padding=\"max_length\", truncation=True, max_length=512)  # Max length set to 512\n",
    "    \n",
    "    # Add labels as a copy of input_ids\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfcc0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_single_turn(examples):\n",
    "    dialogues = []\n",
    "    for dialogue in examples[\"turns\"]:\n",
    "        for i in range(len(dialogue[\"utterance\"]) - 1):\n",
    "            if dialogue[\"speaker\"][i] == 0 and dialogue[\"speaker\"][i + 1] == 1:\n",
    "                user_utterance = dialogue[\"utterance\"][i].strip()\n",
    "                bot_response = dialogue[\"utterance\"][i + 1].strip()\n",
    "                single_turn = (\n",
    "                    f\"[USER] {user_utterance}  [BOT] {bot_response}<|endoftext|>\"\n",
    "                )\n",
    "                dialogues.append(single_turn)\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        dialogues, padding=\"max_length\", truncation=True, max_length=128\n",
    "    )\n",
    "\n",
    "    # Add labels as a copy of input_ids\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e987d-67c7-44ef-b7f0-89d8422f7192",
   "metadata": {},
   "source": [
    "Note that the labels will be shifted by one token to the right inside the GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dbb2786-fd23-4c81-9f95-df67ac36f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train'].map(lambda x: preprocess_data_single_turn(x), batched=True, remove_columns=['dialogue_id', 'services', 'turns'])\n",
    "val_data = dataset['validation'].map(lambda x: preprocess_data_single_turn(x), batched=True, remove_columns=['dialogue_id', 'services', 'turns'])\n",
    "\n",
    "train_data = train_data.shuffle(seed=42)\n",
    "val_data = val_data.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed121e45-cbe3-4da7-8b9b-d3dd74e8b275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 56776\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2139474-f1ce-43d7-97e2-2b37ace87b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 56776 conversations in total.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", train_data.num_rows, \"conversations in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef65cc-f55e-437e-bb68-592497a12fec",
   "metadata": {},
   "source": [
    "Detokenize a few examples from the tokenized train_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74e6e3f7-f9f9-4347-84a9-e720d4a20a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: [USER]  I need to get to bangkok city by 12:00 today. Can you help?   [BOT]  I'd be happy to help! Where are you coming from?<|endoftext|>\n",
      "Example 2: [USER]  Yes. I really would prefer somewhere moderately priced.   [BOT]  There are no moderately priced Korean restaurants. Do you have another food type you would like to try?<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):  # Adjust the range if you want more or fewer examples\n",
    "    input_ids = train_data[i]['input_ids']\n",
    "    \n",
    "    # Filter out the padding tokens manually\n",
    "    input_ids_no_pad = [token_id for token_id in input_ids if token_id != tokenizer.pad_token_id]\n",
    "    \n",
    "    # Detokenize the sequence without the padding tokens\n",
    "    detokenized_sentence = tokenizer.decode(input_ids_no_pad, skip_special_tokens=False)\n",
    "    \n",
    "    print(f\"Example {i + 1}: {detokenized_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc5056",
   "metadata": {},
   "source": [
    "## 2. Understanding Our Model: GPT-2\n",
    "\n",
    "<img src=\"Images/Transformers_graph.jpg\" alt=\"LM Studio logo\" style=\"width: 60%; display: inline-block;\" />\n",
    "\n",
    "\n",
    "Before we start building our chatbot, letâ€™s review what makes GPT-2 a powerful language model for our task.\n",
    "\n",
    "### Why GPT-2?\n",
    "\n",
    "GPT-2 (Generative Pre-trained Transformer 2) is a **decoder-only transformer** model, which means itâ€™s optimized for text generation tasks. Here are some key features:\n",
    "\n",
    "- **Self-Attention Mechanism**: GPT-2 uses self-attention layers to capture relationships between words, allowing it to generate coherent and contextually relevant text.\n",
    "- **Causal Language Modeling**: Itâ€™s trained to predict the next word in a sequence, making it well-suited for conversational applications.\n",
    "- **Pre-training and Fine-tuning**: GPT-2 has been trained on vast amounts of text data, giving it a strong baseline understanding of language (In the orange box). We will fine-tune it with our dataset to make it domain-specific for conversations.\n",
    "\n",
    "**Why fine-tuning?**  \n",
    "Instead of training a model from scratch (which is resource-intensive), we leverage GPT-2's pre-trained knowledge and adapt it for dialogue generation. This significantly reduces the time and data required to achieve good performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "506df0ac-3fa9-4fe9-9016-ee37d0a2312c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50260, 768)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6f871ae-a37a-457c-902f-48831e238929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "GPT2LMHeadModel                                    [1, 12, 512, 64]          --\n",
       "â”œâ”€GPT2Model: 1-1                                   [1, 12, 512, 64]          --\n",
       "â”‚    â””â”€Embedding: 2-1                              [1, 512, 768]             38,599,680\n",
       "â”‚    â””â”€Embedding: 2-2                              [1, 512, 768]             786,432\n",
       "â”‚    â””â”€Dropout: 2-3                                [1, 512, 768]             --\n",
       "â”‚    â””â”€ModuleList: 2-4                             --                        --\n",
       "â”‚    â”‚    â””â”€GPT2Block: 3-1                         [1, 512, 768]             7,087,872\n",
       "â”‚    â”‚    â””â”€GPT2Block: 3-2                         [1, 512, 768]             7,087,872\n",
       "â”‚    â”‚    â””â”€GPT2Block: 3-3                         [1, 512, 768]             7,087,872\n",
       "â”‚    â”‚    â””â”€GPT2Block: 3-4                         [1, 512, 768]             7,087,872\n",
       "â”‚    â”‚    â””â”€GPT2Block: 3-5                         [1, 512, 768]             7,087,872\n",
       "â”‚    â”‚    â””â”€GPT2Block: 3-6                         [1, 512, 768]             7,087,872\n",
       "â”‚    â”‚    â””â”€GPT2Block: 3-7                         [1, 512, 768]             7,087,872\n",
       "â”‚    â”‚    â””â”€GPT2Block: 3-8                         [1, 512, 768]             7,087,872\n",
       "â”‚    â”‚    â””â”€GPT2Block: 3-9                         [1, 512, 768]             7,087,872\n",
       "â”‚    â”‚    â””â”€GPT2Block: 3-10                        [1, 512, 768]             7,087,872\n",
       "â”‚    â”‚    â””â”€GPT2Block: 3-11                        [1, 512, 768]             7,087,872\n",
       "â”‚    â”‚    â””â”€GPT2Block: 3-12                        [1, 512, 768]             7,087,872\n",
       "â”‚    â””â”€LayerNorm: 2-5                              [1, 512, 768]             1,536\n",
       "â”œâ”€Linear: 1-2                                      [1, 512, 50260]           38,599,680\n",
       "====================================================================================================\n",
       "Total params: 163,041,792\n",
       "Trainable params: 163,041,792\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 163.34\n",
       "====================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 630.54\n",
       "Params size (MB): 652.17\n",
       "Estimated Total Size (MB): 1282.71\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the model summary:\n",
    "summary(model, input_data=torch.zeros((1, 512), dtype=torch.long), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89d15658-1351-4e89-995a-fa568385ca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klleshi/miniconda3/envs/NLP/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "# Specify the hyperparameters:\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=PATH_MODELS+\"GPT2chat_single_turn\",  # Directory to save the model checkpoints and other outputs.\n",
    "    max_steps=4000,  # Total number of training steps. The model will stop training once this number is reached.\n",
    "    optim=\"adamw_torch\",  # Optimizer to use during training. 'adamw_torch' refers to AdamW implemented in PyTorch.\n",
    "    learning_rate=5e-5,  # Learning rate used for the optimizer, which controls how much to adjust the weights with respect to the gradient.\n",
    "    weight_decay=0.01,  # Weight decay (L2 regularization) to prevent overfitting by penalizing large weights.\n",
    "    per_device_train_batch_size=16,  # Number of samples per batch for training on each device (e.g., GPU).\n",
    "    per_device_eval_batch_size=16,  # Number of samples per batch for evaluation on each device.\n",
    "    gradient_accumulation_steps=4,  # Number of steps to accumulate gradients before updating model weights, allowing larger effective batch sizes.\n",
    "    gradient_checkpointing=True,  # Save memory by checkpointing gradients, which trades compute time for memory.\n",
    "    warmup_steps=100,  # Number of warmup steps during which the learning rate linearly increases from 0 to the specified value.\n",
    "    lr_scheduler_type=\"linear\",  # Learning rate schedule, with 'linear' meaning it decreases linearly after the warmup phase.\n",
    "    evaluation_strategy=\"steps\",  # Perform evaluation at regular steps, as opposed to other strategies like 'epoch'.\n",
    "    eval_steps=50,  # Number of training steps between evaluations (to check performance on the validation set).\n",
    "    logging_steps=50,  # Number of steps between logging events, used to monitor training progress.\n",
    "    log_level=\"info\",  # The verbosity of logging, 'passive' logging will only occur if you manually enable it.\n",
    "    save_steps=100,  # Number of steps between saving model checkpoints.\n",
    "    save_total_limit=2,  # Maximum number of model checkpoints to keep. Older checkpoints will be deleted when this limit is exceeded.\n",
    "    disable_tqdm=False,  # Whether or not to disable the progress bar ('tqdm'). False means the progress bar will be displayed.\n",
    "    report_to=\"none\",  # This ensures no reporting to any integrations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40e6e049-c13b-4088-9a23-d8b477936f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40ff740c-bc5a-4b26-aee7-c9a0818fe0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_now = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f301ec9-6d6d-4a4a-b905-d716b9ebea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_now:\n",
    "    trainer.train()\n",
    "    model.save_pretrained(PATH_MODELS+\"GPT2chat_single_turn\")\n",
    "    tokenizer.save_pretrained(PATH_MODELS+\"GPT2chat_single_turn\")\n",
    "    log_history = trainer.state.log_history\n",
    "    with open(PATH_MODELS+\"GPT2chat_single_turn/\"+'log_history.json', 'w') as f: json.dump(log_history, f)\n",
    "else:\n",
    "    with open(PATH_MODELS+\"GPT2chat/\"+'log_history.json', 'r') as f: log_history = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e278d-a1e4-4b1e-ab28-d4f10b16f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss:\n",
    "\n",
    "steps = sorted(set(log['step'] for log in log_history if 'step' in log))\n",
    "losses = [log['loss'] for log in log_history if 'loss' in log]\n",
    "val_losses = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(steps, losses, label='Training Loss')\n",
    "plt.plot(steps, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss over Steps')\n",
    "plt.savefig(PATH_RESULTS+\"loss_curves.png\", format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a08ba6-6e07-45b7-a64d-4fa8d463d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_logging.set_verbosity_error()\n",
    "model_name = PATH_MODELS+'GPT2chat_single_turn'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46614715",
   "metadata": {},
   "source": [
    "## Trying Out the Chatbot\n",
    "\n",
    "Now that we have fine-tuned our GPT-2 model, it's time to test it in an interactive way! In this section, we'll create a simple function that allows you to have a conversation with the chatbot. The goal is to observe how the model responds to different prompts and how well it can maintain the context of a conversation.\n",
    "The MultiWOZ dataset contains mostly factual information about booking a hotel room or travelling between cities, so we will ask our fine-tuned GPT2chat to generate short and deterministic answers.\n",
    "\n",
    "### Chat Function Code\n",
    "\n",
    "Here's the Python code that implements the chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d785a9f7-5f49-40b3-807b-72533dfd3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_bot(prompt, previous_chat):\n",
    "    # Format the input with special tokens\n",
    "    input_text = f\"{previous_chat}[USER] {prompt}[BOT]\"\n",
    "\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Truncate previous chats if input exceeds GPT-2's context limit (1024 tokens for GPT-2 base)\n",
    "    if input_ids.shape[-1] > 1024:\n",
    "        input_ids = input_ids[:, -1024:]  # Keep only the last 1024 tokens\n",
    "\n",
    "    # Generate the bot's response\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=20,  \n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids('[PAD]'),\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    # Decode the output, skipping special tokens\n",
    "    response = tokenizer.decode(output[:, input_ids.shape[-1] :][0], skip_special_tokens=True)\n",
    "    \n",
    "    return response, input_text + response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f55a308-bab4-43f2-a1b8-caf2aabe8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start chatting with the bot! Type 'exit' to stop.\")\n",
    "previous_chat = \"\"\n",
    "count = 0\n",
    "while True:\n",
    "    prompt = input(\"You: \")\n",
    "    if prompt.lower() == 'exit':\n",
    "        break\n",
    "    response, previous_chat = chat_with_bot(prompt, previous_chat)\n",
    "    cleaned_response = response.replace(prompt, '').strip() if prompt in response else response \n",
    "    print(f\"GPT2chat: {cleaned_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac7c17-99dd-4615-8820-7e15610f1019",
   "metadata": {},
   "source": [
    "Compare the above fine-tuned GPT2chat with the default pre-trained text generator GPT2text (allowing creative texts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ae5aa-f150-4d82-9128-a8c851357948",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_logging.set_verbosity_error()\n",
    "tokenizer_pretrained = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_pretrained = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model_pretrained.eval()\n",
    "model_pretrained.to(device)\n",
    "\n",
    "def generate_response_pretrained(prompt, model, tokenizer, temperature=0.7):\n",
    "    inputs = tokenizer_pretrained.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model_pretrained.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=100,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    response = tokenizer_pretrained.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c41010b-b24b-498d-a92c-ad4d81b91e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    prompt = input(\"You: \")\n",
    "    if prompt.lower() == 'exit':\n",
    "        break\n",
    "    response = generate_response_pretrained(prompt, model_pretrained, tokenizer_pretrained)\n",
    "    print(f\"GPT2text: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
