{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925cdbd2",
   "metadata": {},
   "source": [
    "# Practical Session: From Pre-trained transformers to Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06846d44",
   "metadata": {},
   "source": [
    "![Chatbot Illustration !](Images/chatbot.jpg \"Designed by www.freepik.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1da823",
   "metadata": {},
   "source": [
    "\n",
    "### Objective\n",
    "\n",
    "Welcome to our hands-on session, **\"From Pre-trained to Chatbot\"**! In this practical session, we'll transform a pre-trained GPT-2 model into a functional chatbot using the `multi_woz_v22` dataset. By the end of this workshop, you will have gained hands-on experience in:\n",
    "\n",
    "- Understanding the mechanics behind a transformer-based chatbot.\n",
    "- Preprocessing and formatting dialogue data for training.\n",
    "- Fine-tuning GPT-2 to generate meaningful, task-oriented dialogue.\n",
    "\n",
    "Ready? Let's dive in!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023c7f1",
   "metadata": {},
   "source": [
    "#### Necessary steps, run the hidden cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbbf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "from transformers import Trainer, TrainingArguments, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import logging as transformers_logging\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from torchinfo import summary\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import numpy\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "!pip install gdown --quiet\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "import gdown\n",
    "from packaging import version\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e082ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm you have the right versions \n",
    "# Minimum required versions\n",
    "required_versions = {\n",
    "    \"python\": \"3.10.15\",\n",
    "    \"torch\": \"2.3.0\",\n",
    "    \"transformers\": \"4.41.2\",\n",
    "    \"datasets\": \"2.19.2\",\n",
    "    \"numpy\": \"1.26.4\"\n",
    "}\n",
    "\n",
    "# Fetch current versions\n",
    "current_versions = {\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"torch\": torch.__version__,\n",
    "    \"transformers\": transformers.__version__,\n",
    "    \"datasets\": datasets.__version__,\n",
    "    \"numpy\": np.__version__\n",
    "}\n",
    "\n",
    "print(\"Version Check:\")\n",
    "for package, required_version in required_versions.items():\n",
    "    current_version = current_versions[package]\n",
    "    \n",
    "    # Compare versions\n",
    "    if version.parse(current_version) >= version.parse(required_version):\n",
    "        print(f\"{package}: {current_version} (✔)\")\n",
    "    else:\n",
    "        print(f\"{package}: {current_version} (✘) - Requires: {required_version} or later\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use GPUs if available:\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Suppress some specific logging and warnings from transformers and Hugging Face Hub modules:\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.file_download\")\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fdd489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed, to make the whole pipeline reproducible:\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "transformers.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad66cb3d",
   "metadata": {},
   "source": [
    "## 1. Our Datasets\n",
    "\n",
    "The `multi_woz_v22` dataset is a comprehensive, multi-domain dialogue dataset, ideal for training task-oriented chatbots. It contains over 10,000 dialogues spanning various domains, such as **restaurant booking, hotel reservations, taxi services, and more**.\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "Each dialogue in [`multi_woz_v22`](https://huggingface.co/datasets/pfb30/multi_woz_v22) contains:\n",
    "- **User Utterances**: Statements or questions from the user, e.g., \"Can you book a hotel for me?\"\n",
    "- **System Responses**: Replies from the system, e.g., \"Sure, which city are you looking for?\"\n",
    "- **Metadata**: Information about the dialogue's domain, state, and other contextual details.\n",
    "\n",
    "In addition to `multi_woz_v22`, we have created our own dataset with conversations featuring a distinct distribution. This smaller dataset complements `multi_woz_v22` and focuses on topics such as jokes, general introductions, and other conversational elements. This dataset can be tailored to meet specific requirements.\n",
    "\n",
    "### Sample Dialogue\n",
    "\n",
    "Below is a code snippet to visualize a sample dialogue from the dataset:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the multi_woz_v22 dataset\n",
    "dataset = load_dataset(\"multi_woz_v22\")\n",
    "print(dataset['train'][0])  # Display the first example in the training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb21349-0166-4bc2-a178-574ecc091b95",
   "metadata": {},
   "source": [
    "We are going to use the Multi-Domain Wizard-of-Oz dataset (MultiWOZ). This is a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. See https://huggingface.co/datasets/pfb30/multi_woz_v22 and https://arxiv.org/abs/1810.00278.\n",
    "\n",
    "Load the \"MultiWOZ\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read our own dataset.\n",
    "with open('../Data/general_conv.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee88a9-39ee-43b9-8123-cc82fbc15cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset('multi_woz_v22', trust_remote_code=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cfc5d6-1cfa-4a31-9014-20626769b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at an example:\n",
    "example = dataset['train'][0]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489c09e-2993-4853-aa18-e493af208a60",
   "metadata": {},
   "source": [
    "### 1.2 Special Tokens and Why They Matter\n",
    "\n",
    "When working with conversational data, it’s crucial to use special tokens to help the model understand the structure of the dialogue. These tokens act as **guidelines** for the model to differentiate between user and system turns and know when the conversation ends.\n",
    "\n",
    "#### Our Special Tokens\n",
    "\n",
    "We'll be using the following special tokens:\n",
    "\n",
    "- **`<|user|>`**: Indicates the beginning of a user utterance.\n",
    "- **`<|system|>`**: Indicates the beginning of a system (chatbot) response.\n",
    "- **`<|endofturn|>`**: Marks the end of each dialogue turn.\n",
    "- **`<|pad|>`**: Used to pad sequences to the same length for batch processing.\n",
    "\n",
    "#### Why Are Special Tokens Important?\n",
    "\n",
    "1. **Clarifying Speaker Roles**: In a conversation, distinguishing who is speaking (user or system) is essential for the model to learn appropriate response patterns.\n",
    "2. **Defining Turn Boundaries**: By marking the end of a dialogue turn, we ensure the model can process and structure conversations more effectively.\n",
    "3. **Efficient Padding**: The `<|pad|>` token ensures that input sequences are uniformly sized, making batch processing efficient and reducing computational load.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d1b60-2427-45b5-87c5-fba8c73d23aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens \n",
    "\n",
    "user=\"<|user|>\"\n",
    "system= \"<|system|>\"\n",
    "pad=\"<|pad|>\"\n",
    "eot=\"<|endofturn|>\"\n",
    "\n",
    "\n",
    "# Visualize a conversation in the dataset.\n",
    "\n",
    "dialogue = example[\"turns\"]\n",
    "\n",
    "for turn_id, speaker, utterance in zip(dialogue[\"turn_id\"], dialogue[\"speaker\"], dialogue[\"utterance\"]):\n",
    "    if speaker == 0:  # User input\n",
    "        print(f\" {user} {utterance.strip()} \")\n",
    "    elif speaker == 1:  # System response\n",
    "        print(f\" {system} {utterance.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5382ffd4-09bb-48ec-8779-e4b0ab67d14f",
   "metadata": {},
   "source": [
    "Load the pre-trained GPT-2 tokenizer and add special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b632d8-92ca-4383-a4e2-35f97a9216d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "special_tokens_dict = {\n",
    "    \"additional_special_tokens\": [\"<|user|>\", \"<|system|>\", \"<|endofturn|>\"],\n",
    "    \"pad_token\": \"<|pad|>\",\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1bcc3-48b1-42f3-8a11-ee242b088479",
   "metadata": {},
   "source": [
    "#### Example Explanation\n",
    "\n",
    "Here the special tokens `'<|user|>'` and `'<|system|>'` help GPT-2 to differentiate between different speakers in a conversation, facilitating the model’s ability to learn how a dialogue typically unfolds. By labeling input text with `'<|user|>'` for the human’s part of the conversation and `'<|system|>'` for the chatbot’s response, you give the model a clear format. This will help it understand conversational roles and can also improve the training of dialogue models.\n",
    "\n",
    "GPT-2 will learn the conversational flow, that is the back-and-forth nature of conversation, and be able to generate coherent dialogue after training. In summary, GPT-2 will generate next words in such a way to mimic a conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8f4df-1b21-45af-bb8a-77c5d438cdf7",
   "metadata": {},
   "source": [
    "Analyse the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae31ed3-79ad-4141-91f0-219e79bf850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = dataset['train']\n",
    "\n",
    "dialogues = []\n",
    "for dialogue in examples['turns']:\n",
    "    dialogue_text = \"\"\n",
    "    for turn_id, speaker, utterance in zip(dialogue[\"turn_id\"], dialogue[\"speaker\"], dialogue[\"utterance\"]):\n",
    "        if speaker == 0:  # User input\n",
    "            dialogue_text += f\"{user} {utterance.strip()} \"\n",
    "        elif speaker == 1:  # System response\n",
    "            dialogue_text += f\"{system} {utterance.strip()} \"\n",
    "    dialogues.append(dialogue_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c78d47-523c-4c08-afe9-c93538568461",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of dialogues in the dataset: \", len(dialogues))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e37095-2c1c-4613-be88-598d1b1e0724",
   "metadata": {},
   "source": [
    "Plot a histogram of the number of tokens in each dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a5c5fd-5cf1-4e36-9983-b066f1e449ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_token_histogram(dialogues, tokenizer, num_bins=10):\n",
    "\n",
    "    # Tokenizing the dialogues and counting tokens in each dialogue\n",
    "    num_tokens_per_dialogue = [len(tokenizer.tokenize(dialogue)) for dialogue in dialogues]\n",
    "\n",
    "    # Create histogram bins\n",
    "    token_min = min(num_tokens_per_dialogue)\n",
    "    token_max = max(num_tokens_per_dialogue)\n",
    "    bins = np.linspace(token_min, token_max, num_bins + 1)  # Create `num_bins` equally spaced bins\n",
    "\n",
    "    # Count occurrences of each token count\n",
    "    token_counts, _ = np.histogram(num_tokens_per_dialogue, bins=bins)\n",
    "\n",
    "    # Count occurrences of [USER] in each dialogue\n",
    "    num_user_tokens_per_dialogue = [dialogue.count(user) for dialogue in dialogues]\n",
    "    user_token_counts = Counter(num_user_tokens_per_dialogue)\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot the histogram of token counts using bins\n",
    "    axs[0].bar(bins[:-1], token_counts, width=np.diff(bins), align='edge')  # Plot with bins\n",
    "    axs[0].set_xlabel('Number of Tokens')\n",
    "    axs[0].set_ylabel('Number of Dialogues')\n",
    "    axs[0].set_title('Histogram of Number of Tokens per Dialogue')\n",
    "\n",
    "    # Plot the histogram of [USER] token counts\n",
    "    axs[1].bar(user_token_counts.keys(), user_token_counts.values())\n",
    "    axs[1].set_xlabel('Number of [USER] Tokens')\n",
    "    axs[1].set_ylabel('Number of Dialogues')\n",
    "    axs[1].set_title(f'Histogram of Number of {user} Tokens per Dialogue')\n",
    "\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_token_histogram(dialogues, tokenizer, num_bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb482dd1-d6a9-470e-992f-985d3fd43fff",
   "metadata": {},
   "source": [
    "Prepare the data in a dialogue format with maximum 256 tokens per conversation (this number may be increased to allow longer conversations) and maximum 8 user-bot exchanges per conversation. \n",
    "\n",
    "Terminology:\n",
    "- 1 turn = 2 messages or one user-bot exchange conversation.\n",
    "- multi-turn = 4-8 turns of user-bot exchange conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66301873-1915-4bb6-99bd-48f0d627c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_singleturn(dialogue):\n",
    "    dialogues=[]\n",
    "    for i in range(0, len(dialogue[\"utterance\"]) - 1, 2):  # Each user-bot exchange is 2 turns\n",
    "        if dialogue[\"speaker\"][i] == 0 and dialogue[\"speaker\"][i + 1] == 1:\n",
    "            user_utterance = dialogue[\"utterance\"][i].strip()\n",
    "            bot_response = dialogue[\"utterance\"][i + 1].strip()\n",
    "            single_turn = f\"{user} {user_utterance} {system} {bot_response} {eot}\"\n",
    "            dialogues.append(single_turn)\n",
    "    return dialogues\n",
    "\n",
    "\n",
    "def extract_multiturn(dialogue,n_turn=4):\n",
    "    context = \"\"  # Persistent context across turns\n",
    "    for id, speaker, utterance in zip(dialogue['turn_id'],dialogue[\"speaker\"], dialogue[\"utterance\"]):\n",
    "        if int(id) < n_turn :\n",
    "            if speaker == 0:  # User input\n",
    "                context += f\"{user} {utterance.strip()} \"\n",
    "            elif speaker == 1:  # System response\n",
    "                if context.strip():  # Ensure there's context to process\n",
    "                    context += f\"{system} {utterance.strip()} {eot}\"  # Append system response to context Add endo of text token\n",
    "                    \n",
    "    return context\n",
    "\n",
    "def extract_general(block,data):\n",
    "    dialogues=[]\n",
    "    dialogue=data[block]\n",
    "    if dialogue[\"speaker\"][0] == 0 and dialogue[\"speaker\"][1] == 1:\n",
    "        user_utterance = dialogue[\"utterance\"][0].strip()\n",
    "        bot_response = dialogue[\"utterance\"][1].strip()\n",
    "        single_turn = f\"{user} {user_utterance} {system} {bot_response} {eot}\"\n",
    "        dialogues.append(single_turn)\n",
    "    return dialogues\n",
    "\n",
    "def extract_text_segment(tokens, max_length, n, tokenizer):\n",
    "    start_idx = (n - 1) * max_length\n",
    "    end_idx = start_idx + max_length\n",
    "\n",
    "    segment_tokens = tokens[start_idx:end_idx]\n",
    "\n",
    "    # Decode back to text\n",
    "    segment_text = tokenizer.decode(segment_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Pad if necessary\n",
    "    if len(segment_tokens) < max_length:\n",
    "        pad_length = max_length - len(segment_tokens)\n",
    "        pad_token_id = tokenizer.convert_tokens_to_ids(pad)\n",
    "        segment_tokens.extend([pad_token_id] * pad_length)\n",
    "        segment_text = tokenizer.decode(segment_tokens, skip_special_tokens=True)\n",
    "    return segment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_balanced(examples):\n",
    "\n",
    "    dialogues = []\n",
    "    block=1\n",
    "    for dialogue in examples['turns']:\n",
    "        single_list=extract_singleturn(dialogue)\n",
    "        dialogues.extend(single_list)\n",
    "        # extract general conversations.\n",
    "        if block<len(data):\n",
    "            general_list=extract_general(block=block,data=data)\n",
    "            dialogues.extend(general_list)\n",
    "        if block%2==0:\n",
    "            multi_list=extract_multiturn(dialogue,n_turn=8)\n",
    "            dialogues.append(multi_list)\n",
    "        block+=1\n",
    "   \n",
    "    tokenized_inputs = tokenizer(\n",
    "        dialogues, padding=\"max_length\", truncation=True, max_length=256\n",
    "    )\n",
    "\n",
    "    # Add labels as a copy of input_ids\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e987d-67c7-44ef-b7f0-89d8422f7192",
   "metadata": {},
   "source": [
    "Note that the labels will be shifted by one token to the right inside the GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb2786-fd23-4c81-9f95-df67ac36f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train'].map(lambda x: preprocess_data_balanced(x), batched=True, remove_columns=['dialogue_id', 'services', 'turns'])\n",
    "val_data = dataset['validation'].map(lambda x: preprocess_data_balanced(x), batched=True, remove_columns=['dialogue_id', 'services', 'turns'])\n",
    "\n",
    "train_data = train_data.shuffle(seed=42)\n",
    "val_data = val_data.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed121e45-cbe3-4da7-8b9b-d3dd74e8b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2139474-f1ce-43d7-97e2-2b37ace87b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", train_data.num_rows, \"conversations in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef65cc-f55e-437e-bb68-592497a12fec",
   "metadata": {},
   "source": [
    "Detokenize a few examples from the tokenized train_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e6e3f7-f9f9-4347-84a9-e720d4a20a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):  # Adjust the range if you want more or fewer examples\n",
    "    input_ids = train_data[i]['input_ids']\n",
    "    \n",
    "    # Filter out the padding tokens manually\n",
    "    input_ids_no_pad = [token_id for token_id in input_ids if token_id != tokenizer.pad_token_id]\n",
    "    \n",
    "    # Detokenize the sequence without the padding tokens\n",
    "    detokenized_sentence = tokenizer.decode(input_ids_no_pad, skip_special_tokens=False)\n",
    "    \n",
    "    print(f\"Example {i + 1}: {detokenized_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc5056",
   "metadata": {},
   "source": [
    "## 2. Understanding Our Model: GPT-2\n",
    "\n",
    "<img src=\"Images/Transformers_graph.jpg\" alt=\"LM Studio logo\" style=\"width: 60%; display: inline-block;\" />\n",
    "\n",
    "\n",
    "Before we start building our chatbot, let’s review what makes GPT-2 a powerful language model for our task.\n",
    "\n",
    "### Why GPT-2?\n",
    "\n",
    "GPT-2 (Generative Pre-trained Transformer 2) is a **decoder-only transformer** model, which means it’s optimized for text generation tasks. Here are some key features:\n",
    "\n",
    "- **Self-Attention Mechanism**: GPT-2 uses self-attention layers to capture relationships between words, allowing it to generate coherent and contextually relevant text.\n",
    "- **Causal Language Modeling**: It’s trained to predict the next word in a sequence, making it well-suited for conversational applications.\n",
    "- **Pre-training and Fine-tuning**: GPT-2 has been trained on vast amounts of text data, giving it a strong baseline understanding of language (In the orange box). We will fine-tune it with our dataset to make it domain-specific for conversations.\n",
    "\n",
    "**Why fine-tuning?**  \n",
    "Instead of training a model from scratch (which is resource-intensive), we leverage GPT-2's pre-trained knowledge and adapt it for dialogue generation. This significantly reduces the time and data required to achieve good performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506df0ac-3fa9-4fe9-9016-ee37d0a2312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f871ae-a37a-457c-902f-48831e238929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary:\n",
    "summary(model, input_data=torch.zeros((1, 512), dtype=torch.long), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d15658-1351-4e89-995a-fa568385ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the hyperparameters:\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"GPT2chat_single_turn\",  # Directory to save the model checkpoints and other outputs.\n",
    "    max_steps=4000,  # Total number of training steps. The model will stop training once this number is reached.\n",
    "    optim=\"adamw_torch\",  # Optimizer to use during training. 'adamw_torch' refers to AdamW implemented in PyTorch.\n",
    "    learning_rate=5e-5,  # Learning rate used for the optimizer, which controls how much to adjust the weights with respect to the gradient.\n",
    "    weight_decay=0.01,  # Weight decay (L2 regularization) to prevent overfitting by penalizing large weights.\n",
    "    per_device_train_batch_size=16,  # Number of samples per batch for training on each device (e.g., GPU).\n",
    "    per_device_eval_batch_size=16,  # Number of samples per batch for evaluation on each device.\n",
    "    gradient_accumulation_steps=4,  # Number of steps to accumulate gradients before updating model weights, allowing larger effective batch sizes.\n",
    "    gradient_checkpointing=True,  # Save memory by checkpointing gradients, which trades compute time for memory.\n",
    "    warmup_steps=100,  # Number of warmup steps during which the learning rate linearly increases from 0 to the specified value.\n",
    "    lr_scheduler_type=\"linear\",  # Learning rate schedule, with 'linear' meaning it decreases linearly after the warmup phase.\n",
    "    evaluation_strategy=\"steps\",  # Perform evaluation at regular steps, as opposed to other strategies like 'epoch'.\n",
    "    eval_steps=50,  # Number of training steps between evaluations (to check performance on the validation set).\n",
    "    logging_steps=50,  # Number of steps between logging events, used to monitor training progress.\n",
    "    log_level=\"info\",  # The verbosity of logging, 'passive' logging will only occur if you manually enable it.\n",
    "    save_steps=100,  # Number of steps between saving model checkpoints.\n",
    "    save_total_limit=2,  # Maximum number of model checkpoints to keep. Older checkpoints will be deleted when this limit is exceeded.\n",
    "    disable_tqdm=False,  # Whether or not to disable the progress bar ('tqdm'). False means the progress bar will be displayed.\n",
    "    report_to=\"none\",  # This ensures no reporting to any integrations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e6e049-c13b-4088-9a23-d8b477936f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff740c-bc5a-4b26-aee7-c9a0818fe0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_now = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f301ec9-6d6d-4a4a-b905-d716b9ebea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_now:\n",
    "    trainer.train()\n",
    "    model.save_pretrained(\"GPT2chat_single_turn\")\n",
    "    tokenizer.save_pretrained(\"GPT2chat_single_turn\")\n",
    "    log_history = trainer.state.log_history\n",
    "    with open(\"GPT2chat_single_turn/\"+'log_history.json', 'w') as f: json.dump(log_history, f)\n",
    "else:\n",
    "    # Define the file ID and output file name\n",
    "    folder_id='1kSMbzOsvGu-AQwj4NOxu1ZjNVXiGBsSu'\n",
    "    output_folder = 'model'  # Name to save the file locally\n",
    "\n",
    "    # Use gdown to download the file from Google Drive\n",
    "\n",
    "    # Download the folder\n",
    "    !gdown --folder https://drive.google.com/drive/folders/$folder_id -O $output_folder\n",
    "    # Load the model and tokenizer directly from the downloaded file\n",
    "\n",
    "    # Load the trained model and tokenizer\n",
    "    model_path = f'./{output_folder}'  # Path to the downloaded model file\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    print(\"Model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e278d-a1e4-4b1e-ab28-d4f10b16f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss:\n",
    "if train_now:\n",
    "    steps = sorted(set(log['step'] for log in log_history if 'step' in log))\n",
    "    losses = [log['loss'] for log in log_history if 'loss' in log]\n",
    "    val_losses = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(steps, losses, label='Training Loss')\n",
    "    plt.plot(steps, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss over Steps')\n",
    "    plt.savefig(\"loss_curves.png\", format='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46614715",
   "metadata": {},
   "source": [
    "## Trying Out the Chatbot\n",
    "\n",
    "Now that we have fine-tuned our GPT-2 model, it's time to test it in an interactive way! In this section, we'll create a simple function that allows you to have a conversation with the chatbot. The goal is to observe how the model responds to different prompts and how well it can maintain the context of a conversation.\n",
    "The MultiWOZ dataset contains mostly factual information about booking a hotel room or travelling between cities, so we will ask our fine-tuned GPT2chat to generate short and deterministic answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model is in evaluation mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def chat_with_bot(input_text, previous_chat,count,max_length=256):\n",
    "    # Encode the input text based on whether it's the first message or not\n",
    "    if count == 0:\n",
    "        input_text = f\"{user} {input_text}{system}\"\n",
    "    else:\n",
    "        input_text = f\"{previous_chat}{user} {input_text}{system}\"\n",
    "    \n",
    "    # print(\"INPUT: \",count, input_text)\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "    # Generate the response\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.convert_tokens_to_ids(eot),\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Decode the output, skipping special tokens\n",
    "    response = tokenizer.decode(\n",
    "        output[:, input_ids.shape[-1] :][0], skip_special_tokens=True\n",
    "    )\n",
    "    return response, input_text + response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f55a308-bab4-43f2-a1b8-caf2aabe8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start chatting with the bot! Type 'exit' to stop.\")\n",
    "previous_chat = \"\"\n",
    "count = 0\n",
    "while True:\n",
    "    prompt = input(\"You: \")\n",
    "    if prompt.lower() == 'exit':\n",
    "        break\n",
    "    response, previous_chat = chat_with_bot(prompt, previous_chat,count)\n",
    "    count+=1\n",
    "    cleaned_response = response.replace(prompt, '').strip() if prompt in response else response \n",
    "    print(f\"GPT2chat: {cleaned_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac7c17-99dd-4615-8820-7e15610f1019",
   "metadata": {},
   "source": [
    "Compare the above fine-tuned GPT2chat with the default pre-trained text generator GPT2text (allowing creative texts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ae5aa-f150-4d82-9128-a8c851357948",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_logging.set_verbosity_error()\n",
    "tokenizer_pretrained = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_pretrained = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model_pretrained.eval()\n",
    "model_pretrained.to(device)\n",
    "\n",
    "def generate_response_pretrained(prompt, model, tokenizer, temperature=0.7):\n",
    "    inputs = tokenizer_pretrained.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model_pretrained.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=100,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    response = tokenizer_pretrained.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c41010b-b24b-498d-a92c-ad4d81b91e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    prompt = input(\"You: \")\n",
    "    if prompt.lower() == 'exit':\n",
    "        break\n",
    "    response = generate_response_pretrained(prompt, model_pretrained, tokenizer_pretrained)\n",
    "    print(f\"GPT2text: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
